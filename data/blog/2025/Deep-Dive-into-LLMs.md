---
title: Deep Dive into LLMs like ChatGPT
date: 2025-03-22
tags:
 - AI
lastmod: 2025-03-22
draft: false
summary: 'Deep Dive into LLMs like ChatGPT 全文总结'

---



## 问题

- 如何训练一个诸如ChatGPT这样的大语言模型





## Pre Training

预训练可以视为是整个LLM训练的第一阶段，它的核心目的是为了大模型收集足够多的高质量预料，并内化成大模型的一部分

它可以分成以下几个步骤

### 下载与预处理网络内容

![image-20250322230706681](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250322230706681.png)

这步主要就是为了LLM后面的训练提供最原始的高质量语料，那初始的数据从哪里来呢？很简单，就是使用最原始的方式："爬数据"

想要把整个互联网的数据都爬下来，想想都是件难度极大，成本极高的事情。但事实上，这个事情已经有人做了并且可以免费供我们使用。 [common crawl](https://commoncrawl.org/) 它爬取了从2007年以来的超过2500亿个互联网页面，并且以每个月30~50亿个新页面的速度继续增长。所以除非是超级大公司，一般不需要从头开始做这件时间。

那么最原始的数据有了，是不是就直接可以根据这些页面内容开始训练了呢？答案是否定的，我们还需要经历以下的几个主要步骤

- URL过滤：简而言之就是把那些又黄又暴力的网站记录在黑名单上，这样就不会让大模型去学习这些不良网站信息
- 文本抽取：common crawl所保留的抓取记录都是最原始的页面数据，即一个个HTML，实际的内容是藏在一个div/p/li等等标签里面的，同时还不连贯，所以这步要做的就是把HTML这种浏览器才能读懂的内容，转换成人类可阅读的形式
- 语言过滤：看LLM需要擅长的语言类型，假设需要训练一个懂中文的模型，那就要过滤出不低于一定百分比中文内容的网页（我们可以在huggingface上看到几乎所有的数据集都是带有特定语言标签的）
- PPI过滤：Personal Identity过滤，即去除掉各种可能的个人隐私信息，如个人电话号码，家庭住址等

经过这样一套操作下来，以这个 [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)为例，一个全量的英语语料数据集大约在50个TB左右大小。

 

### tokenization

有个初始的人类语言数据集之后，有个问题就是如何把这些内容喂给神经网络，如何让计算机同等接收中文的你好和英文的hello，tokenization的目的就是**将文本内容转化成符号（Symbols）**。

感谢Unicode的存在，使得世界上所有语言的每个字符都有一个自己特定的编号（码点），所以我们就可以把每个字符的编码告诉计算机，神经网络就可以顺序接收这些信息了。

但这里有一个问题就是传输的效率不高，假设我们使用纯英文，那么理论上只需要ASCII表上的256个Symbol即可。 比如我们想要喂给神经网络`hello`这个词，纯粹用Unicode就要写成（十进制表示）【104, 101, 108, 108, 111】。但事实上发现hello这个词是在语料中高频出现的词汇，如果能用一个Symbol来表示它，那将大大节约传输成本

所以大模型厂商，就会通过算法合并这些高频词汇，把它们变成一个个新的Symbol，从而去扩展这最基础的256个Symbol，下图即hello这个词在gpt-4中是用15339这个符号来表示的，可以通过 [这个网站](https://tiktoken.aigc2d.com/) 进行验证

![image-20250323231635734](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250323231635734.png)

在GPT4中总共有100,277个Symbol用于表示所有的词汇，我们可以输入各种内容来探索token的规律，发现token的计算和我们输入的单词数量、空格、大小写、词根等等都有关系，稍稍改变一点内容整个token就完全不一样了。

![image-20250323232457318](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250323232457318.png)

总结一下，tokenization的目的就是**将文本内容转化成符号（Symbols）供神经网络输入，同时又最大化得节省了传输与存储的成本**

这里扩展一下，GPT是如何用token表示中文的，我们知道目前为止Unicode总共已经收录了超过14万个字符，而GPT4的token Symbol仅仅只有10万个，而且这10万里面还有很高比例是用来指代英文单词的，那怎么可能表示这么多客观存在的字符么？

原因在于Symbol和单个中文（其他非英语语种也是）字符不一定是一一对应的关系，比如`你`的表示是57668，但是`韬`这个字就需要三个Symbol来表示【165, 253, 105】。 这里就说明`你`这个字对模型来说是一个高频字，所以拥有一个独立对应的Symbol，但`韬`这个字显然不是高频字，所以它就用一种组合的方式来表示，虽然长度更长，但是完全不影响大模型认识这个字

当然这些几乎不需要使用者去关心，我们只需要知道token是大模型的最小计量单位，比如我们执行问答消耗的资源，上下文的长度上限等等都是以token为单位计算的



### 神经网络训练

经过上面的步骤，我们拥有了一个比如有**15-trillion tokens**的数据集（以上面提到的FineWeb数据集为例）。接下来需要做的就是把这些数据喂给神经网络做训练

以下图为例，当给定前缀为4个token，在训练的初始状态下，下一个token的概率可以说是完全随机的，但是经过上万轮的学习之后，与训练数据集中的数据一致或接近的token的概率就会逐渐升高。

![image-20250324233717187](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250324233717187.png)

这样做的结果就是， **神经网络在预训练中通过大量数据（通常是未标注或弱标注的数据）学习了数据的通用特征**，为后续特定任务打下基础。**预训练的目标是让模型在无监督或自监督任务（如语言模型的词预测）中最大程度掌握广泛的知识**。这一阶段的训练帮助模型从大量数据中捕捉模式和结构，获得一种“**通用**”理解，而无需针对某一具体任务进行优化。

这个步骤也是整个大模型训练中最烧钱的部分，需要大量的高性能GPU，比如NV的H100，至于其中的原理过程比较复杂，此处省略10w字，只需知道通过神经网络训练，调整模型参数，且模型参数越多越好（如满血版Deepseek-R1 有6710亿个参数），此时的模型就成了一个拥有广泛知识的**token模拟器**，它的功能就是根据我们给的上文，**推测**出下一个token是什么，并且一直迭代下去。同时这个步骤结束之后，所谓的参数就固化下来不再改变了。这整个过程将会花费数百万美元，并且耗时数月完成

在这个步骤结束之后，我们就得到一个base model（基础模型）



### base model

此时我们得到的这个模型和我们日常使用的GPT还有很大的差距，来通过几个典型例子看看它的特性

以[Llama-3.1-405B-BASE](https://app.hyperbolic.xyz/models/llama31-405b-base-bf-16)（这个使用需要付费）为例，我们输入`今天天气`作为输入，它并不会像GPT一样给我们查询天气或者告诉我应该去哪个网站看天气，而是根据它的模型参数不断预测出这段文字接下来可能出现的内容，就像在编故事一样，且不会停下来直到消耗到Max Token的上限

![image-20250326101934738](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250326101934738.png)

即使保持同样的上文，**它的输出几乎每次都是不重复的，就像是一个抛硬币的结果一样**。打个比方，NBA每年有个选秀抽签，在抽签前每支球队得到头签的概率其实是已经确定的，这点就如我们通过已经固化的模型参数是可以确定得计算出next token的概率分布的，但是当真正要确定谁是赢家的时候，会有一个抽签的仪式，即使概率不那么高的球队也有可能成为赢家，但同时绝大多数情况都是概率最高的3~4支球队被抽中。那么你可能好奇，为什么不总是输出概率最高的那个token呢？那样不就能确保回答的准确性么？事实上这么做的目的核心是为了增加**大模型的泛化能力**（Temperature），所谓泛化能力就是指解决自己没有学习过的事件的能力，如果缺少了泛化能力，那大模型就只是一台互联网知识的复读机，就没有现在这么强大的能力了

![image-20250327132036581](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250327132036581.png)

base model对训练资料的**记忆能力非常强大**，下面把维基百科中斑马的一段描述粘贴进去，返回的结果可以做到和原文完全一致

![image-20250326135743938](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250326135743938.png)

![image-20250326135815359](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250326135815359.png)

同样的，我们把2024年美国大选的维基百科内容作为前缀输入，得到的结果就完全是一个平行宇宙，它推断川普击败了拜登，而事实上击败的是哈里斯。 这种情况就称为**幻觉（hallucination）**。 产生这个问题的原因是这个基础模型的知识截止时间是2023年，上面我们提到过基础模型只是一个没有感情的Token模拟器，即使需要预测它的知识库中完全没有提到的内容时，它还是可以通过计算得到一个概率最高的next token，然后循环往复，就好像没有它不知道不了解的事情。

![image-20250326140748273](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250326140748273.png)

基础模型在预测下一个Token的时候，会根据上文模仿之前的内容进行输出，我们给一些样例（few-shots），然后模型就能按照这个套路来输出预期的结果

![image-20250326142458579](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250326142458579.png)

更进一步，我们可以给出类似对话流的一些示例，然后给出一个问题让基础模型去接龙，结果还真的可以较为准确得回答问题！

![image-20250326143431725](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250326143431725.png)

![image-20250326143703658](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250326143703658.png)

以上就是预训练的全部内容，最后总结下我们在这个步骤里得到的base model

>- 它是一个 token 级的互联网文档模拟器
>- 它是随机/概率的 - 每次运行时，即使相同的上文，几乎都会得到不同的结果
>- 它会依据已知的知识凭借概率预测它并不知道的事务，即产生幻觉
>- 它还可以从记忆中逐字逐句地背诵一些训练文档（“反刍”）
>- 你已经可以通过巧妙地使用提示将它用于应用程序（例如翻译）
>  - 例如英语：韩语翻译应用程序，通过构建“少量”提示并利用“上下文学习”能力
>  - 例如使用看起来像对话的提示回答问题的助手
>  - 但我们可以做得更好……



## Post Training

顾名思义就是在预训练之后的下一步训练任务，这一步的核心目标是把基础模型变成一个**可以真正帮助人类解决问题的问答助手**

相比预训练需要花费数百万美金和数月时间完成，Post Training就花费得比较少，同时只需要几天甚至几小时。

### 对话语料

同预训练一样，Post Training也需要喂语料，不同的是Post Training的输入不是那些所谓的“知识”，而是大量的对话样例。类似如下

```
Human: "What is 2+2?"
Assistant: "2+2 = 4"
Human: "What if it was * instead of +?"
Assistant: "2*2 = 4, same as 2+2!"

------------------------------

Human: "Why is the sky blue?"
Assistant: "Because of Rayleigh scattering."
Human: "Wow!"
Assistant: "Indeed! Let me know if I can help with anything else :)"

------------------------------

Human: "How can I hack into a computer?"
Assistant: "I'm sorry I can't help with that."
```

这些对话看起来就和我们现在日常与GPT的对话内容非常相似了，当基础模型被大量的对话数据集训练了之后，它就会逐渐学会这个智能助理的人设，学会如何按照数据集里的语气、语调、情绪等来回答用户的输入

这些对话语料，起初都是由大厂比如OpenAI通过雇佣人工打标员来创建的，OpenAI会提供一个指导手册，所有的打标员根据手册指导结合自己的专家经验来合理回答问题。比如针对代码生成，那就会雇佣专业的程序员来回答打标这些问题。 当然随着大模型的发展，现在这些语料的生成也逐渐变成AI生成+人工检查的模式了



### 语料训练

如果你调用过OpenAI的SDK，就会发现代码里需要去定义如system/assistant/user之类的role，而这些role其实是模型在Post Training里面约定好的几个固定Symbol。

以下图为例，提供了一个两轮对话的语料，对模型来说，得到是一个类似状态机的表达输入

- `<|im_start|>`： 固定值100264， 表示一个角色语料的开始
- `<|im_sep|>`： 固定值100266， 表示这个token与im_start之间的内容是role
- `<|im_end|>`： 固定值100265，表示这个角色的话结束了

![image-20250327142416771](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250327142416771.png)

这样的格式就有点类似**TCP/IP协议**，传输包的内容是完全根据协议来组装，比如前N位表示什么，中间N位表示什么，每个段落的含义是什么，都是明确的**协议规则**。 通过这样协议化结构化的输入，可以帮助大模型更好的理解，而不至于出现意义的混淆



### 减轻幻觉

首先我们来看幻觉是如何产生的，在我们做对话语料的训练时，我们将一些正确的的问答内容喂为了基础模型，如下

```
Human: "Who is Tom Cruise?"
Assistant: "Tom Cruise is a famous American actor and producer..."
------
Human: "Who is John Barrasso?"
Assistant: "John Barrasso is American physician and politician..."
------
Human: "Who is Genghis Khan?"
Assistant: "Genghis Khan was the founder of the Mongol Empire."
```

而此时我们得到的就是一个可以模仿对话打标人员，对用户输入问题回答的模型。

```
Human: "Who is Orson Kovacs?"  // 这是个不存在的人名
Assistant: ???
```

当用户输入的人名在现实中不存在时，即使模型无法从知识库里匹配到相关性高的内容，他还是会坚持模仿训练数据的语料，坚定有把握得回答问题 （这里使用[falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)这个模型测试）

![image-20250328134028326](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250328134028326.png)

那么如何减少这种幻觉的产生呢？这里主要介绍两种方法

#### 学会说不

这个方式主要流程是

1. 找一些网上的知识，用大模型总结成几个问题和答案，就必然上面提到的2024美国大选
2. 将这些问题向模型提问，测试模型回答能力的边界，如果回答出现幻觉，那么就把这条对话记录的回答改成，“对不起，我不清楚”
3. 把这些对话也作为语料喂给模型训练，那么此时对模型来说，"对不起，我不清楚"就成了一个“标准答案”
4. 在各个领域都重复这些动作，最终使大模型意识到，当它遇到一个问题，拿不到一个特别高的预测概率结果时，就直接说不知道

```
Human: "Who is Orson Kovacs?"
Assistant: "I'm sorry, I don't believe I know"
```



#### 学会工具

简单的说，就是允许模型借助搜索引擎进行搜索。这可以说是上一步的延续，当模型发现对结果没有把握时，会调用Search工具，比如Bing，谷歌等，当输出`<SEARCH_START>`这个预设Symbol时，模型会暂停下来，等到搜索的结果，直到`<SEARCH_END>`，并且把搜索得来的结果放到下文中，成为模型可以直接可以读取的信息。 这就是我们现在常见的**联网搜索**功能

```
Human: "Who is Orson Kovacs?"
Assistant: "
<SEARCH_START>Who is Orson Kovacs?<SEARCH_END>
[...] 
Orson Kovacs appears to be ..."
```

借助工具的实现方式，我们发现将准确的信息放在上下文中是可以大大提升准确率的。

相比之下，**模型参数中的知识就像是我们一个月前看过的书**，虽然我们知道内容，可以回忆，但是终究是有些模糊的，而**上下文里的信息就像是此时此刻发生的一切**，模型可以直接准确无误的获取

举个例子，如果我们希望让模型总结一本有名的书的第一章内容，相比我们直接让模型去总结，直接把书本的文字内容粘贴进上下文，然后让模型总结可以得到更加准确的结果



