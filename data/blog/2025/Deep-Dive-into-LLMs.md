---
title: Deep Dive into LLMs like ChatGPT
date: 2025-03-22
tags:
 - AI
lastmod: 2025-03-22
draft: false
summary: 'Deep Dive into LLMs like ChatGPT 全文总结'

---



## 问题

- 如何训练一个诸如ChatGPT这样的大语言模型





## 预训练

预训练可以视为是整个LLM训练的第一阶段，它的核心目的是为了大模型收集足够多的高质量预料，并内化成大模型的一部分

它可以分成以下几个步骤

### 下载与预处理网络内容

![image-20250322230706681](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250322230706681.png)

这步主要就是为了LLM后面的训练提供最原始的高质量语料，那初始的数据从哪里来呢？很简单，就是使用最原始的方式："爬数据"

想要把整个互联网的数据都爬下来，想想都是件难度极大，成本极高的事情。但事实上，这个事情已经有人做了并且可以免费供我们使用。 [common crawl](https://commoncrawl.org/) 它爬取了从2007年以来的超过2500亿个互联网页面，并且以每个月30~50亿个新页面的速度继续增长。所以除非是超级大公司，一般不需要从头开始做这件时间。

那么最原始的数据有了，是不是就直接可以根据这些页面内容开始训练了呢？答案是否定的，我们还需要经历以下的几个主要步骤

- URL过滤：简而言之就是把那些又黄又暴力的网站记录在黑名单上，这样就不会让大模型去学习这些不良网站信息
- 文本抽取：common crawl所保留的抓取记录都是最原始的页面数据，即一个个HTML，实际的内容是藏在一个div/p/li等等标签里面的，同时还不连贯，所以这步要做的就是把HTML这种浏览器才能读懂的内容，转换成人类可阅读的形式
- 语言过滤：看LLM需要擅长的语言类型，假设需要训练一个懂中文的模型，那就要过滤出不低于一定百分比中文内容的网页
- PPI过滤：Personal Identity过滤，即去除掉各种可能的个人隐私信息，如个人电话号码，家庭住址等

经过这样一套操作下来，以这个 [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)为例，一个全量的英语语料数据集大约在50个TB左右大小。

 

### tokenization

有个初始的人类语言数据集之后，有个问题就是如何把这些内容喂给神经网络，如何让计算机同等接收中文的你好和英文的hello，tokenization的目的就是**将文本内容转化成符号（Symbols）**。

感谢Unicode的存在，使得世界上所有语言的每个字符都有一个自己特定的编号（码点），所以我们就可以把每个字符的编码告诉计算机，神经网络就可以顺序接收这些信息了。

但这里有一个问题就是传输的效率不高，假设我们使用纯英文，那么理论上只需要ASCII表上的256个Symbol即可。 比如我们想要喂给神经网络`hello`这个词，纯粹用Unicode就要写成（十进制表示）【104, 101, 108, 108, 111】。但事实上发现hello这个词是在语料中高频出现的词汇，如果能用一个Symbol来表示它，那将大大节约传输成本

所以大模型厂商，就会通过算法合并这些高频词汇，把它们变成一个个新的Symbol，从而去扩展这最基础的256个Symbol，下图即hello这个词在gpt-4中是用15339这个符号来表示的，可以通过 [这个网站](https://tiktoken.aigc2d.com/) 进行验证

![image-20250323231635734](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250323231635734.png)

在GPT4中总共有100,277个Symbol用于表示所有的词汇，我们可以输入各种内容来探索token的规律，发现token的计算和我们输入的单词数量、空格、大小写、词根等等都有关系，稍稍改变一点内容整个token就完全不一样了。当然这个几乎不需要使用者去关心。

![image-20250323232457318](https://kuimo-markdown-pic.oss-cn-hangzhou.aliyuncs.com/image-20250323232457318.png)

总结一下，tokenization的目的就是**将文本内容转化成符号（Symbols）供神经网络输入，同时又最大化得节省了传输与存储的成本**

这里扩展一下，GPT是如何用token表示中文的，我们知道目前为止Unicode总共已经收录了超过14万个字符，而GPT4的token Symbol仅仅只有10万个，而且这10万里面还有很高比例是用来指代英文单词的，那怎么可能表示这么多客观存在的字符么？

原因在于Symbol和单个中文（其他非英语语种也是）字符不一定是一一对应的关系，比如`你`的表示是57668，但是`韬`这个字就需要三个Symbol来表示【165, 253, 105】。 这里就说明`你`这个字对模型来说是一个高频字，所以拥有一个独立对应的Symbol，但`韬`这个字显然不是高频字，所以它就用一种组合的方式来表示，虽然长度更长，但是完全不影响大模型认识这个字











## 词汇表

